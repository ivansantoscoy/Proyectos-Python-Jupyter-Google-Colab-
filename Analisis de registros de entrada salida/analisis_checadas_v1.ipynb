{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3aff519",
   "metadata": {},
   "source": [
    "# Bloque 0 – Introducción y diccionario de columnas\n",
    "\n",
    "Este cuaderno analiza errores de checada para determinar si los patrones observados sugieren mayor probabilidad de error humano o falla de sistema. \n",
    "\n",
    "El flujo incluye:\n",
    "- Limpieza de datos\n",
    "- Estadística (Poisson/U-Chart)\n",
    "- Pareto\n",
    "- Clustering\n",
    "- Cruces por torniquete/turno\n",
    "- Cruces extendidos\n",
    "- PCA opcional\n",
    "- Resumen interpretativo de la hipótesis\n",
    "\n",
    "**Diccionario del dataset normalizado:**\n",
    "- `fecha`: Día del evento.\n",
    "- `dia_semana`: Día de la semana derivado de `fecha`.\n",
    "- `turno`: Turno operativo (1=matutino, 3=nocturno, etc.).\n",
    "- `empresa`: Código extraído de `Trabajador` (47 o 506).\n",
    "- `torniquete`: 1 o 2 cuando es físico; 0 cuando se asigna manualmente (pareja manual en ambos lados).\n",
    "- `entradas`: Número de checadas válidas (swipes) en torniquete 1/2.\n",
    "- `empleados_unicos`: Personas distintas registradas.\n",
    "- `manual_parcial`: Entrada o salida manual, la otra en torniquete (pareja completa con un lado manual).\n",
    "- `manual_ambos`: Entrada y salida manual (pareja completa, ambos lados manuales).\n",
    "- `anomalia_incompleta`: Falta de entrada o salida (no hay pareja completa).\n",
    "- `manuales`: Suma de manual_parcial + manual_ambos.\n",
    "- `anomalias`: Alias de `anomalia_incompleta`.\n",
    "- `justificados`: Casos con terminal vacía y concepto (VAC, FJ, SUS). Informativo, no es error.\n",
    "- `errores`: Total de errores (manual_parcial + manual_ambos + anomalia_incompleta).\n",
    "- `errores_por_empleado`: Errores / empleados únicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934ad46",
   "metadata": {},
   "source": [
    "## Preparación (librerías y función para títulos de gráficos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def _ensure(pkg):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"])\n",
    "\n",
    "for p in [\"pandas\",\"numpy\",\"xlsxwriter\",\"scipy\",\"scikit-learn\",\"matplotlib\"]:\n",
    "    _ensure(p)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "FORCE_DELIMITER = None\n",
    "DATE_KWARGS = dict(errors=\"coerce\", dayfirst=False)  # mm/dd/yyyy\n",
    "N_CLUSTERS = 3\n",
    "\n",
    "def show_with_spaced_title(title):\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\" + title)  # 2 renglones en blanco y título en el tercero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394bf4c",
   "metadata": {},
   "source": [
    "## Input de archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d999a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = input(\"Escribe el nombre del archivo CSV (ej. datos.csv): \").strip()\n",
    "print(\"Usando archivo:\", FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe1531",
   "metadata": {},
   "source": [
    "## Funciones auxiliares (detección de encabezado, mapeo de columnas y clasificación neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54271326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_header_row_and_delimiter(path, search_limit=200, force_delim=None):\n",
    "    with open(path, \"r\", encoding=\"latin1\") as f:\n",
    "        lines = f.readlines()\n",
    "    header_candidates = []\n",
    "    for i, line in enumerate(lines[:search_limit]):\n",
    "        lower = line.lower()\n",
    "        if all(k in lower for k in [\"empresa\",\"trabajador\",\"fecha\",\"checada\",\"turno\",\"terminal\"]):\n",
    "            header_candidates.append(i)\n",
    "    header_row = header_candidates[0] if header_candidates else None\n",
    "    if force_delim:\n",
    "        delim = force_delim\n",
    "    else:\n",
    "        try:\n",
    "            seg = \"\".join(lines[(header_row or 0): (header_row or 0) + 5])\n",
    "            delim = csv.Sniffer().sniff(seg).delimiter\n",
    "        except Exception:\n",
    "            delim = \",\"\n",
    "    return header_row, delim\n",
    "\n",
    "def pick(colnames, *aliases):\n",
    "    colnames = [str(c) for c in colnames]\n",
    "    for a in aliases:\n",
    "        if a in colnames: return a\n",
    "    simp = {c.lower().replace(\" \",\"\").replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\"): c for c in colnames}\n",
    "    for a in aliases:\n",
    "        key = a.lower().replace(\" \",\"\").replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "        if key in simp: return simp[key]\n",
    "    return None\n",
    "\n",
    "def simplify_tipo(s):\n",
    "    t = s.astype(str).str.lower()\n",
    "    return np.where(t.str.contains(\"entrada\"), \"Entrada\",\n",
    "           np.where(t.str.contains(\"salida\"), \"Salida\", \"\"))\n",
    "\n",
    "def split_empresa_empleado(trabajador):\n",
    "    tr = trabajador.astype(str).str.strip()\n",
    "    empresa = np.where(tr.str.startswith(\"47\"), \"47\", np.where(tr.str.startswith(\"506\"), \"506\", \"NA\"))\n",
    "    empleado_id = tr.str.replace(r\"^(47|506)\", \"\", regex=True)\n",
    "    return empresa, empleado_id\n",
    "\n",
    "def base_flags(df):\n",
    "    df[\"is_valid_swipe\"] = df[\"torniquete\"].isin([1,2]) & df[\"tipo_simpl\"].isin([\"Entrada\",\"Salida\"])\n",
    "    df[\"is_manual\"] = df[\"torniquete\"].fillna(-999).astype(int).eq(0)\n",
    "    return df\n",
    "\n",
    "def classify_neutral(group):\n",
    "    is_e_val = ((group[\"tipo_simpl\"]==\"Entrada\") & group[\"is_valid_swipe\"]).any()\n",
    "    is_s_val = ((group[\"tipo_simpl\"]==\"Salida\")  & group[\"is_valid_swipe\"]).any()\n",
    "    is_e_man = ((group[\"tipo_simpl\"]==\"Entrada\") & group[\"is_manual\"]).any()\n",
    "    is_s_man = ((group[\"tipo_simpl\"]==\"Salida\")  & group[\"is_manual\"]).any()\n",
    "\n",
    "    e_terms = group.loc[(group[\"tipo_simpl\"]==\"Entrada\") & group[\"is_valid_swipe\"], \"torniquete\"].dropna().unique()\n",
    "    s_terms = group.loc[(group[\"tipo_simpl\"]==\"Salida\")  & group[\"is_valid_swipe\"], \"torniquete\"].dropna().unique()\n",
    "\n",
    "    pair_complete = ((is_e_val or is_e_man) and (is_s_val or is_s_man))\n",
    "\n",
    "    manual_parcial = 0\n",
    "    manual_ambos = 0\n",
    "    anomalia_incompleta = 0\n",
    "    tor_attr = 0\n",
    "\n",
    "    if pair_complete:\n",
    "        if is_e_man and is_s_man:\n",
    "            manual_ambos = 1\n",
    "            tor_attr = 0\n",
    "        elif is_e_man and is_s_val:\n",
    "            manual_parcial = 1\n",
    "            tor_attr = int(s_terms[0]) if len(s_terms)>0 else 0\n",
    "        elif is_s_man and is_e_val:\n",
    "            manual_parcial = 1\n",
    "            tor_attr = int(e_terms[0]) if len(e_terms)>0 else 0\n",
    "        else:\n",
    "            tor_attr = int(e_terms[0]) if len(e_terms)>0 else (int(s_terms[0]) if len(s_terms)>0 else 0)\n",
    "    else:\n",
    "        anomalia_incompleta = 1\n",
    "        any_terms = np.concatenate([e_terms, s_terms])\n",
    "        tor_attr = int(any_terms[0]) if any_terms.size>0 else 0\n",
    "\n",
    "    return pd.Series({\"manual_parcial\": manual_parcial,\n",
    "                      \"manual_ambos\": manual_ambos,\n",
    "                      \"anomalia_incompleta\": anomalia_incompleta,\n",
    "                      \"torniquete_attr\": tor_attr})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81794ee",
   "metadata": {},
   "source": [
    "# Bloque 1 – Limpieza y normalización\n",
    "\n",
    "- Detecta encabezados y delimitador.\n",
    "- Interpreta fechas en mm/dd/aaaa.\n",
    "- Separa empresa y empleado.\n",
    "- Clasificación neutral: `manual_parcial`, `manual_ambos`, `anomalia_incompleta`.\n",
    "- Agregación por día–turno–torniquete–empresa.\n",
    "- Exportación opcional a CSV/XLSX (comentado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e90849",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_row, delim = detect_header_row_and_delimiter(FILE_PATH, force_delim=FORCE_DELIMITER)\n",
    "if header_row is None:\n",
    "    df_raw = pd.read_csv(FILE_PATH, encoding=\"latin1\", sep=(delim or \",\"), on_bad_lines=\"skip\")\n",
    "else:\n",
    "    df_raw = pd.read_csv(FILE_PATH, encoding=\"latin1\", sep=(delim or \",\"), header=header_row, on_bad_lines=\"skip\")\n",
    "df_raw.columns = [str(c).strip() for c in df_raw.columns]\n",
    "\n",
    "cols = df_raw.columns.tolist()\n",
    "col_trabajador  = pick(cols, \"Trabajador\",\"trabajador\",\"num empleado\",\"empleado\")\n",
    "col_fecha       = pick(cols, \"Fecha\",\"fecha\")\n",
    "col_checada     = pick(cols, \"Checada\",\"checada\",\"FechaHora\",\"fechahora\")\n",
    "col_tipo        = pick(cols, \"TipoRegistro\",\"tipo registro\",\"tiporegistro\",\"tipo\")\n",
    "col_turno       = pick(cols, \"Turno\",\"turno\")\n",
    "col_terminal    = pick(cols, \"Terminal\",\"terminal\")\n",
    "col_concepto    = pick(cols, \"Concepto\",\"concepto\",\"Motivo\",\"motivo\")\n",
    "\n",
    "use_cols = [c for c in [col_trabajador, col_fecha, col_checada, col_tipo, col_turno, col_terminal, col_concepto] if c is not None]\n",
    "df = df_raw[use_cols].copy()\n",
    "\n",
    "if col_fecha in df.columns:\n",
    "    df[\"fecha\"] = pd.to_datetime(df[col_fecha], **DATE_KWARGS).dt.date\n",
    "else:\n",
    "    df[\"fecha\"] = pd.to_datetime(df[col_checada], **DATE_KWARGS).dt.date\n",
    "\n",
    "if col_checada in df.columns:\n",
    "    df[\"checada_dt\"] = pd.to_datetime(df[col_checada], **DATE_KWARGS)\n",
    "else:\n",
    "    df[\"checada_dt\"] = pd.NaT\n",
    "\n",
    "if col_turno in df.columns:\n",
    "    df[\"turno\"] = (df[col_turno].astype(str).str.extract(r\"(\\d+)\")[0].astype(float).astype(\"Int64\"))\n",
    "else:\n",
    "    df[\"turno\"] = pd.NA\n",
    "\n",
    "if col_terminal in df.columns:\n",
    "    df[\"terminal_raw\"] = df[col_terminal].astype(str).str.strip()\n",
    "    df[\"torniquete\"] = pd.to_numeric(df[\"terminal_raw\"].replace({\"\": np.nan}), errors=\"coerce\")\n",
    "else:\n",
    "    df[\"torniquete\"] = np.nan\n",
    "\n",
    "if col_concepto in df.columns:\n",
    "    df[\"concepto\"] = df[col_concepto].astype(str).str.strip()\n",
    "else:\n",
    "    df[\"concepto\"] = \"\"\n",
    "\n",
    "if col_tipo in df.columns:\n",
    "    df[\"tipo_simpl\"] = simplify_tipo(df[col_tipo])\n",
    "else:\n",
    "    df[\"tipo_simpl\"] = \"\"\n",
    "\n",
    "if col_trabajador in df.columns:\n",
    "    df[\"empresa\"], df[\"empleado_id\"] = split_empresa_empleado(df[col_trabajador])\n",
    "else:\n",
    "    df[\"empresa\"] = \"NA\"\n",
    "    df[\"empleado_id\"] = \"\"\n",
    "\n",
    "df = base_flags(df)\n",
    "\n",
    "par = (df.groupby([\"fecha\",\"empresa\",\"empleado_id\",\"turno\"], dropna=False)\n",
    "         .apply(classify_neutral)\n",
    "         .reset_index())\n",
    "\n",
    "entradas = (df.loc[df[\"is_valid_swipe\"]]\n",
    "              .groupby([\"fecha\",\"turno\",\"torniquete\",\"empresa\"], dropna=False)\n",
    "              .agg(entradas=(\"empleado_id\",\"count\"),\n",
    "                   empleados_unicos=(\"empleado_id\",\"nunique\"))\n",
    "              .reset_index())\n",
    "\n",
    "mp = (par[par[\"manual_parcial\"]==1]\n",
    "      .groupby([\"fecha\",\"turno\",\"empresa\",\"torniquete_attr\"])\n",
    "      .size().rename(\"manual_parcial\").reset_index().rename(columns={\"torniquete_attr\":\"torniquete\"}))\n",
    "\n",
    "ma = (par[par[\"manual_ambos\"]==1]\n",
    "      .assign(torniquete_attr=0)\n",
    "      .groupby([\"fecha\",\"turno\",\"empresa\",\"torniquete_attr\"])\n",
    "      .size().rename(\"manual_ambos\").reset_index().rename(columns={\"torniquete_attr\":\"torniquete\"}))\n",
    "\n",
    "an = (par[par[\"anomalia_incompleta\"]==1]\n",
    "      .groupby([\"fecha\",\"turno\",\"empresa\",\"torniquete_attr\"])\n",
    "      .size().rename(\"anomalia_incompleta\").reset_index().rename(columns={\"torniquete_attr\":\"torniquete\"}))\n",
    "\n",
    "from functools import reduce\n",
    "componentes = [entradas, mp, ma, an]\n",
    "base = reduce(lambda a,b: pd.merge(a,b, on=[\"fecha\",\"turno\",\"torniquete\",\"empresa\"], how=\"outer\"), componentes)\n",
    "\n",
    "for c in [\"entradas\",\"empleados_unicos\",\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]:\n",
    "    if c not in base.columns: base[c] = 0\n",
    "base[[\"entradas\",\"empleados_unicos\",\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]] =     base[[\"entradas\",\"empleados_unicos\",\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]].fillna(0).astype(int)\n",
    "\n",
    "justificados = (df.loc[df[\"torniquete\"].isna() & df[\"concepto\"].ne(\"\")]\n",
    "                  .groupby([\"fecha\",\"turno\",\"empresa\"], dropna=False)\n",
    "                  .agg(justificados=(\"empleado_id\",\"count\"))\n",
    "                  .reset_index())\n",
    "base = base.merge(justificados, on=[\"fecha\",\"turno\",\"empresa\"], how=\"left\")\n",
    "base[\"justificados\"] = base[\"justificados\"].fillna(0).astype(int)\n",
    "\n",
    "base[\"manuales\"] = base[\"manual_parcial\"] + base[\"manual_ambos\"]\n",
    "base[\"anomalias\"] = base[\"anomalia_incompleta\"]\n",
    "base[\"errores\"] = base[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]].sum(axis=1)\n",
    "base[\"errores_por_empleado\"] = np.where(base[\"empleados_unicos\"]>0, base[\"errores\"]/base[\"empleados_unicos\"], np.nan)\n",
    "base[\"dia_semana\"] = pd.to_datetime(base[\"fecha\"]).dt.day_name()\n",
    "\n",
    "final_cols = [\"fecha\",\"dia_semana\",\"turno\",\"empresa\",\"torniquete\",\n",
    "              \"entradas\",\"empleados_unicos\",\n",
    "              \"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\",\n",
    "              \"manuales\",\"anomalias\",\"justificados\",\"errores\",\"errores_por_empleado\"]\n",
    "base = base[final_cols].sort_values([\"fecha\",\"turno\",\"empresa\",\"torniquete\"])\n",
    "\n",
    "print(\"Preview normalizado:\")\n",
    "print(base.head(10))\n",
    "\n",
    "# Exportación opcional:\n",
    "# base.to_csv(\"dataset_normalizado_turnos_torniquetes.csv\", index=False, encoding=\"utf-8\")\n",
    "# with pd.ExcelWriter(\"dataset_normalizado_turnos_torniquetes.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "#     base.to_excel(writer, index=False, sheet_name=\"Datos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111d04a",
   "metadata": {},
   "source": [
    "# Bloque 2 – Exploración inicial\n",
    "\n",
    "Exploramos errores totales por día para una visión general de la variabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f74f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = base.copy()\n",
    "resumen_dia = df.groupby(\"fecha\")[\"errores\"].sum().reset_index()\n",
    "print(\"Errores totales por día:\")\n",
    "print(resumen_dia)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(resumen_dia[\"fecha\"], resumen_dia[\"errores\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Fecha\"); plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por día\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70068a9",
   "metadata": {},
   "source": [
    "# Bloque 3 – Distribución de Poisson y U-Chart\n",
    "\n",
    "Se compara la distribución observada de errores diarios con una Poisson (λ = media diaria) y se grafica un U-Chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_diario = resumen_dia[\"errores\"].mean()\n",
    "print(f\"Lambda (media diaria de errores) = {lambda_diario:.2f}\")\n",
    "\n",
    "# Histograma observado\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(resumen_dia[\"errores\"], bins=15, rwidth=0.85, density=True)\n",
    "plt.xlabel(\"Errores en un día\"); plt.ylabel(\"Densidad\")\n",
    "show_with_spaced_title(\"Distribución observada de errores diarios\")\n",
    "\n",
    "# Poisson teórica\n",
    "max_error = int(resumen_dia[\"errores\"].max())\n",
    "x = np.arange(0, max_error+1)\n",
    "pmf = stats.poisson.pmf(x, lambda_diario)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x, pmf, \"o-\")\n",
    "plt.xlabel(\"Errores en un día\"); plt.ylabel(\"Probabilidad\")\n",
    "show_with_spaced_title(f\"Poisson teórica (λ={lambda_diario:.1f})\")\n",
    "\n",
    "# U-Chart (simplificado)\n",
    "n = len(resumen_dia)\n",
    "ucl = lambda_diario + 3*np.sqrt(lambda_diario/n)\n",
    "lcl = max(0,  lambda_diario - 3*np.sqrt(lambda_diario/n))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(resumen_dia[\"fecha\"], resumen_dia[\"errores\"], marker=\"o\", linestyle=\"-\", label=\"Observado\")\n",
    "plt.axhline(lambda_diario, linestyle=\"--\", label=\"Media\")\n",
    "plt.axhline(ucl, linestyle=\"--\", label=\"UCL\")\n",
    "plt.axhline(lcl, linestyle=\"--\", label=\"LCL\")\n",
    "plt.xticks(rotation=45); plt.xlabel(\"Fecha\"); plt.ylabel(\"Errores\")\n",
    "plt.legend()\n",
    "show_with_spaced_title(\"U-Chart de errores diarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2d60f",
   "metadata": {},
   "source": [
    "# Bloque 4 – Pareto de causas\n",
    "\n",
    "Diagrama de Pareto para priorizar el tipo de error que más contribuye.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "causas = {\n",
    "    \"Manual parcial\": df[\"manual_parcial\"].sum(),\n",
    "    \"Manual ambos\": df[\"manual_ambos\"].sum(),\n",
    "    \"Anomalía incompleta\": df[\"anomalia_incompleta\"].sum()\n",
    "}\n",
    "df_pareto = pd.DataFrame(list(causas.items()), columns=[\"Causa\",\"Total\"]).sort_values(\"Total\", ascending=False)\n",
    "print(df_pareto)\n",
    "\n",
    "df_pareto[\"Porcentaje\"] = df_pareto[\"Total\"] / df_pareto[\"Total\"].sum() * 100\n",
    "df_pareto[\"Acumulado\"] = df_pareto[\"Porcentaje\"].cumsum()\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.bar(df_pareto[\"Causa\"], df_pareto[\"Total\"])\n",
    "ax1.set_ylabel(\"Errores (total)\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_pareto[\"Causa\"], df_pareto[\"Acumulado\"], marker=\"o\")\n",
    "ax2.set_ylabel(\"% Acumulado\"); ax2.set_ylim(0,110)\n",
    "show_with_spaced_title(\"Pareto de causas de error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01bab5",
   "metadata": {},
   "source": [
    "# Bloque 5 – Clustering (KMeans)\n",
    "\n",
    "Segmentamos días por composición de errores (proporciones). Se anotan centroides (C0, C1, ...) como \"tooltips\" mínimos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01899a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clust = df.groupby(\"fecha\")[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\",\"errores\"]].sum().reset_index()\n",
    "df_clust[\"p_manual_parcial\"]  = df_clust[\"manual_parcial\"] / df_clust[\"errores\"].replace(0,1)\n",
    "df_clust[\"p_manual_ambos\"]    = df_clust[\"manual_ambos\"]   / df_clust[\"errores\"].replace(0,1)\n",
    "df_clust[\"p_anomalia_incomp\"] = df_clust[\"anomalia_incompleta\"] / df_clust[\"errores\"].replace(0,1)\n",
    "\n",
    "X = df_clust[[\"p_manual_parcial\",\"p_manual_ambos\",\"p_anomalia_incomp\"]].fillna(0)\n",
    "scaler = StandardScaler(); X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "df_clust[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"Cluster asignado por día (primeras filas):\")\n",
    "print(df_clust.head())\n",
    "\n",
    "# Proyección 2D simple (primeras 2 variables estandarizadas)\n",
    "x_axis = X_scaled[:,0]  # p_manual_parcial\n",
    "y_axis = X_scaled[:,1]  # p_manual_ambos\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for c in sorted(df_clust[\"cluster\"].unique()):\n",
    "    subset = df_clust[df_clust[\"cluster\"]==c]\n",
    "    plt.scatter(x_axis[subset.index], y_axis[subset.index], label=f\"Cluster {c}\", alpha=0.7)\n",
    "\n",
    "for i, ctr in enumerate(centroids):\n",
    "    plt.scatter(ctr[0], ctr[1], marker=\"X\", s=120)\n",
    "    plt.annotate(f\"C{i}\", (ctr[0], ctr[1]), textcoords=\"offset points\", xytext=(6,6))\n",
    "\n",
    "plt.xlabel(\"p_manual_parcial (estandarizado)\")\n",
    "plt.ylabel(\"p_manual_ambos (estandarizado)\")\n",
    "plt.legend()\n",
    "show_with_spaced_title(\"Clustering de días (centroides anotados)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750189f2",
   "metadata": {},
   "source": [
    "# Bloque 5.4–5.8 – Lectura avanzada de clusters\n",
    "\n",
    "Perfil promedio y cruces por turno, torniquete y empresa. Incluye resumen textual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfil_cluster = (df_clust.groupby(\"cluster\")[[\"p_manual_parcial\",\"p_manual_ambos\",\"p_anomalia_incomp\"]]\n",
    "                   .mean().reset_index().sort_values(\"cluster\"))\n",
    "print(\"Perfil promedio por cluster:\")\n",
    "print(perfil_cluster)\n",
    "\n",
    "df_clusters_join = df.merge(df_clust[[\"fecha\",\"cluster\"]], on=\"fecha\", how=\"left\")\n",
    "\n",
    "crosstab_turno = (df_clusters_join.groupby([\"cluster\",\"turno\"])[\"errores\"].sum()\n",
    "                  .reset_index().pivot(index=\"cluster\", columns=\"turno\", values=\"errores\").fillna(0))\n",
    "crosstab_torniquete = (df_clusters_join.groupby([\"cluster\",\"torniquete\"])[\"errores\"].sum()\n",
    "                       .reset_index().pivot(index=\"cluster\", columns=\"torniquete\", values=\"errores\").fillna(0))\n",
    "crosstab_empresa = (df_clusters_join.groupby([\"cluster\",\"empresa\"])[\"errores\"].sum()\n",
    "                    .reset_index().pivot(index=\"cluster\", columns=\"empresa\", values=\"errores\").fillna(0))\n",
    "\n",
    "crosstab_turno.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por cluster y turno\")\n",
    "\n",
    "crosstab_torniquete.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por cluster y torniquete\")\n",
    "\n",
    "crosstab_empresa.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por cluster y empresa\")\n",
    "\n",
    "def resumen_cluster(perfil, ct_turno, ct_torn, ct_emp):\n",
    "    print(\"\\n=== Resumen de clusters ===\")\n",
    "    for c in perfil[\"cluster\"].tolist():\n",
    "        p = perfil[perfil[\"cluster\"]==c][[\"p_manual_parcial\",\"p_manual_ambos\",\"p_anomalia_incomp\"]].iloc[0]\n",
    "        print(f\"\\nCluster {c}: manual_parcial={p['p_manual_parcial']:.2f}, manual_ambos={p['p_manual_ambos']:.2f}, anomalia={p['p_anomalia_incomp']:.2f}\")\n",
    "        if c in ct_turno.index:\n",
    "            fila = ct_turno.loc[c].fillna(0); print(f\"  Turno dominante: {fila.idxmax()} (total={int(fila.max())})\")\n",
    "        if c in ct_torn.index:\n",
    "            fila = ct_torn.loc[c].fillna(0); print(f\"  Torniquete dominante: {fila.idxmax()} (total={int(fila.max())})\")\n",
    "        if c in ct_emp.index:\n",
    "            fila = ct_emp.loc[c].fillna(0); print(f\"  Empresa dominante: {fila.idxmax()} (total={int(fila.max())})\")\n",
    "resumen_cluster(perfil_cluster, crosstab_turno, crosstab_torniquete, crosstab_empresa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2d2e3",
   "metadata": {},
   "source": [
    "# Bloque 6 – Cruces por torniquete y turno\n",
    "\n",
    "Buscamos concentraciones por torniquete × turno y por tipo de error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_torn_turno = (df.groupby([\"torniquete\",\"turno\"])[\"errores\"].sum()\n",
    "                         .reset_index().pivot(index=\"torniquete\", columns=\"turno\", values=\"errores\").fillna(0))\n",
    "print(\"Errores por torniquete y turno:\")\n",
    "print(crosstab_torn_turno)\n",
    "\n",
    "crosstab_torn_turno.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por torniquete × turno\")\n",
    "\n",
    "crosstab_torn_tipo = (df.groupby([\"torniquete\"])[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]]\n",
    "                        .sum().reset_index().set_index(\"torniquete\"))\n",
    "crosstab_torn_tipo.plot(kind=\"bar\", stacked=True, figsize=(8,5))\n",
    "plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por torniquete según tipo\")\n",
    "\n",
    "crosstab_turno_tipo = (df.groupby([\"turno\"])[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]]\n",
    "                        .sum().reset_index().set_index(\"turno\"))\n",
    "crosstab_turno_tipo.plot(kind=\"bar\", stacked=True, figsize=(8,5))\n",
    "plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por turno según tipo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee44288",
   "metadata": {},
   "source": [
    "# Bloque 7 – Cruces extendidos\n",
    "\n",
    "Errores por empresa, día de la semana y detección de simultaneidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a54a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "errores_empresa = (df.groupby(\"empresa\")[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\",\"errores\"]]\n",
    "                     .sum().reset_index())\n",
    "print(\"Errores por empresa:\")\n",
    "print(errores_empresa)\n",
    "\n",
    "errores_dia_sem = (df.groupby(\"dia_semana\")[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\",\"errores\"]]\n",
    "                     .sum().reset_index().set_index(\"dia_semana\"))\n",
    "errores_dia_sem.plot(kind=\"bar\", figsize=(9,5))\n",
    "plt.ylabel(\"Errores\")\n",
    "show_with_spaced_title(\"Errores por día de la semana\")\n",
    "\n",
    "simul = (df.groupby([\"fecha\",\"turno\",\"torniquete\"])[\"errores\"].sum().reset_index())\n",
    "simul = simul.sort_values(\"errores\", ascending=False).head(15)\n",
    "print(\"Top 15 combinaciones con mayor simultaneidad de errores (día–turno–torniquete):\")\n",
    "print(simul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a49e8",
   "metadata": {},
   "source": [
    "# Bloque 8 – PCA opcional\n",
    "\n",
    "PCA de variables de error por día. Visualización 2D con anotaciones (medianas por cluster).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b2eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_src = df_clust[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]].fillna(0)\n",
    "scaler_pca = StandardScaler(); X_pca_scaled = scaler_pca.fit_transform(X_pca_src)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_pca_scaled)\n",
    "df_clust[\"PCA1\"] = X_pca[:,0]; df_clust[\"PCA2\"] = X_pca[:,1]\n",
    "\n",
    "print(\"Varianza explicada por componente:\", pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(df_clust[\"PCA1\"], df_clust[\"PCA2\"], c=df_clust[\"cluster\"], cmap=\"viridis\", alpha=0.85)\n",
    "for c in sorted(df_clust[\"cluster\"].unique()):\n",
    "    med1 = df_clust.loc[df_clust[\"cluster\"]==c, \"PCA1\"].median()\n",
    "    med2 = df_clust.loc[df_clust[\"cluster\"]==c, \"PCA2\"].median()\n",
    "    plt.scatter(med1, med2, marker=\"X\", s=120)\n",
    "    plt.annotate(f\"C{c}\", (med1, med2), textcoords=\"offset points\", xytext=(6,6))\n",
    "plt.xlabel(\"PCA1\"); plt.ylabel(\"PCA2\")\n",
    "show_with_spaced_title(\"PCA (2 componentes) coloreado por cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d58327",
   "metadata": {},
   "source": [
    "# Bloque 9 – Resumen final e interpretación\n",
    "\n",
    "Resumen de hallazgos y guía para interpretar si el patrón sugiere error humano o falla de sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd969f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_por_tipo = df[[\"manual_parcial\",\"manual_ambos\",\"anomalia_incompleta\"]].sum()\n",
    "print(\"Totales por tipo de error:\")\n",
    "print(total_por_tipo)\n",
    "\n",
    "concentracion_torn_turno = (df.groupby([\"torniquete\",\"turno\"])[\"errores\"].sum().reset_index()\n",
    "                              .sort_values(\"errores\", ascending=False).head(10))\n",
    "print(\"\\nTop combinaciones torniquete×turno por errores:\")\n",
    "print(concentracion_torn_turno)\n",
    "\n",
    "texto = []\n",
    "max_tt = concentracion_torn_turno[\"errores\"].max() if not concentracion_torn_turno.empty else 0\n",
    "if max_tt > 0:\n",
    "    texto.append(\"- Se observan combinaciones torniquete×turno con alta concentración de errores; revisar hardware/red/configuración en esas combinaciones.\")\n",
    "if total_por_tipo.get(\"manual_ambos\",0) > total_por_tipo.get(\"manual_parcial\",0) and total_por_tipo.get(\"manual_ambos\",0) > total_por_tipo.get(\"anomalia_incompleta\",0):\n",
    "    texto.append(\"- Predomina 'manual_ambos' (parejas totalmente manuales), lo cual sugiere posible problema de sistema en ciertos periodos.\")\n",
    "elif total_por_tipo.get(\"manual_parcial\",0) >= max(total_por_tipo.get(\"manual_ambos\",0), total_por_tipo.get(\"anomalia_incompleta\",0)):\n",
    "    texto.append(\"- Predomina 'manual_parcial' (solo un lado manual). Esto suele estar más asociado a prácticas del personal, pero validar cruces y reincidencia.\")\n",
    "else:\n",
    "    texto.append(\"- 'anomalia_incompleta' es relevante; si se concentra por torniquete/turno puede ser técnico; si se concentra por empleado, puede ser operativo.\")\n",
    "\n",
    "print(\"\\nResumen interpretativo:\")\n",
    "for line in texto:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\nConclusión sugerida:\")\n",
    "print(\"Use los cruces por torniquete×turno, simultaneidad por día y reincidencia por empleado para decidir:\")\n",
    "print(\"- Si los errores se concentran en un torniquete/turno y afectan a muchos empleados a la vez, la probabilidad apunta a falla de sistema.\")\n",
    "print(\"- Si los errores están dispersos por torniquetes/turnos pero concentrados en ciertos empleados o horarios específicos, la probabilidad apunta a prácticas humanas.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
